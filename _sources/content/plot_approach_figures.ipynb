{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e516d6e6",
   "metadata": {},
   "source": [
    "# Generate tedana walkthrough figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f2027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/site-packages/nilearn/datasets/__init__.py:96: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2356/2619228700.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Install the data if running locally, or point to cached data if running on neurolibre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mDATA_REQ_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../binder/data_requirement.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Download data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os.path as op\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting, masking\n",
    "from nilearn.image import math_img\n",
    "from repo2data.repo2data import Repo2Data\n",
    "\n",
    "# Install the data if running locally, or point to cached data if running on neurolibre\n",
    "DATA_REQ_FILE = os.path.join(\"../binder/data_requirement.json\")\n",
    "\n",
    "# Download data\n",
    "repo2data = Repo2Data(DATA_REQ_FILE)\n",
    "data_path = repo2data.install()\n",
    "data_path = os.path.abspath(os.path.join(data_path[0], \"data\"))\n",
    "\n",
    "ted_dir = os.path.join(data_path, \"tedana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd66c1",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea92e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_dir = os.path.join(data_path, \"sub-04570/func/\")\n",
    "data_files = [\n",
    "    os.path.join(func_dir, \"sub-04570_task-rest_echo-1_space-scanner_desc-partialPreproc_bold.nii.gz\"),\n",
    "    os.path.join(func_dir, \"sub-04570_task-rest_echo-2_space-scanner_desc-partialPreproc_bold.nii.gz\"),\n",
    "    os.path.join(func_dir, \"sub-04570_task-rest_echo-3_space-scanner_desc-partialPreproc_bold.nii.gz\"),\n",
    "    os.path.join(func_dir, \"sub-04570_task-rest_echo-4_space-scanner_desc-partialPreproc_bold.nii.gz\"),\n",
    "]\n",
    "echo_times = [12., 28., 44., 60.]\n",
    "\n",
    "adaptive_mask_file = op.join(ted_dir, \"sub-04570_task-rest_space-scanner_desc-adaptiveGoodSignal_mask.nii.gz\")\n",
    "mask = math_img('img >= 3', img=adaptive_mask_file)\n",
    "\n",
    "# Optimally combined data\n",
    "oc = masking.apply_mask(op.join(ted_dir, \"sub-04570_task-rest_space-scanner_desc-optcom_bold.nii.gz\"), mask)\n",
    "oc_z = (oc - np.mean(oc, axis=0)) / np.std(oc, axis=0)\n",
    "\n",
    "# Results from MEPCA\n",
    "mepca_mmix = np.loadtxt(op.join(ted_dir, \"sub-04570_task-rest_space-scanner_desc-PCA_mixing.tsv\"))\n",
    "oc_red = masking.apply_mask(op.join(ted_dir, \"sub-04570_task-rest_space-scanner_desc-optcomPCAReduced_bold.nii.gz\"), mask)\n",
    "\n",
    "# Results from MEICA\n",
    "meica_mmix = np.loadtxt(op.join(ted_dir, \"sub-04570_task-rest_space-scanner_desc-ICA_mixing.tsv\"))\n",
    "norm_weights = masking.apply_mask(op.join(ted_dir, \"sub-04570_task-rest_space-scanner_desc-ICAAveragingWeights_components.nii.gz\"), mask)\n",
    "meica_betas = masking.apply_mask(\n",
    "    [\n",
    "        op.join(ted_dir, \"sub-04570_task-rest_space-scanner_echo-1_desc-ICA_components.nii.gz\"),\n",
    "        op.join(ted_dir, \"sub-04570_task-rest_space-scanner_echo-2_desc-ICA_components.nii.gz\"),\n",
    "        op.join(ted_dir, \"sub-04570_task-rest_space-scanner_echo-3_desc-ICA_components.nii.gz\"),\n",
    "        op.join(ted_dir, \"sub-04570_task-rest_space-scanner_echo-4_desc-ICA_components.nii.gz\"),\n",
    "    ],\n",
    "    mask,\n",
    ")\n",
    "r2_pred_betas = masking.apply_mask(\n",
    "    [\n",
    "        op.join(ted_dir, \"sub-04570_task-rest_space-scanner_echo-1_desc-ICAT2ModelPredictions_components.nii.gz\"),\n",
    "        op.join(ted_dir, \"sub-04570_task-rest_space-scanner_echo-2_desc-ICAT2ModelPredictions_components.nii.gz\"),\n",
    "        op.join(ted_dir, \"sub-04570_task-rest_space-scanner_echo-3_desc-ICAT2ModelPredictions_components.nii.gz\"),\n",
    "        op.join(ted_dir, \"sub-04570_task-rest_space-scanner_echo-4_desc-ICAT2ModelPredictions_components.nii.gz\"),\n",
    "    ],\n",
    "    mask,\n",
    ")\n",
    "s0_pred_betas = masking.apply_mask(\n",
    "    [\n",
    "        op.join(ted_dir, \"sub-04570_task-rest_space-scanner_echo-1_desc-ICAS0ModelPredictions_components.nii.gz\"),\n",
    "        op.join(ted_dir, \"sub-04570_task-rest_space-scanner_echo-2_desc-ICAS0ModelPredictions_components.nii.gz\"),\n",
    "        op.join(ted_dir, \"sub-04570_task-rest_space-scanner_echo-3_desc-ICAS0ModelPredictions_components.nii.gz\"),\n",
    "        op.join(ted_dir, \"sub-04570_task-rest_space-scanner_echo-4_desc-ICAS0ModelPredictions_components.nii.gz\"),\n",
    "    ],\n",
    "    mask,\n",
    ")\n",
    "\n",
    "# Component betas\n",
    "betas_file = op.join(ted_dir, \"sub-04570_task-rest_space-scanner_desc-ICA_components.nii.gz\")\n",
    "beta_maps = masking.apply_mask(betas_file, mask)\n",
    "\n",
    "# Multi-echo denoised data\n",
    "dn_data = masking.apply_mask(op.join(ted_dir, \"sub-04570_task-rest_space-scanner_desc-optcomDenoised_bold.nii.gz\"), mask)\n",
    "hk_data = masking.apply_mask(op.join(ted_dir, \"sub-04570_task-rest_space-scanner_desc-optcomAccepted_bold.nii.gz\"), mask)\n",
    "\n",
    "# Post-processed data\n",
    "dn_t1c_data = masking.apply_mask(op.join(ted_dir, \"sub-04570_task-rest_space-scanner_desc-optcomMIRDenoised_bold.nii.gz\"), mask)\n",
    "hk_t1c_data = masking.apply_mask(op.join(ted_dir, \"sub-04570_task-rest_space-scanner_desc-optcomAccepted_bold.nii.gz\"), mask)\n",
    "\n",
    "# Get voxel index for most related to first component (checkerboard)\n",
    "voxel_idx = np.where(beta_maps[0, :] == np.max(beta_maps[0, :]))[0][0]\n",
    "\n",
    "# load data\n",
    "data = [masking.apply_mask(f, mask) for f in data_files]\n",
    "ts = [d[:, voxel_idx] for d in data]\n",
    "ts_1d = np.hstack(ts)\n",
    "\n",
    "n_echoes = len(data)\n",
    "n_trs = data[0].shape[0]\n",
    "\n",
    "# Component table\n",
    "df = pd.read_table(\n",
    "    op.join(ted_dir, \"sub-04570_task-rest_space-scanner_desc-tedana_metrics.tsv\"),\n",
    "    index_col=\"component\",\n",
    ")\n",
    "\n",
    "pal = sns.color_palette('cubehelix', n_echoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232271e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for model\n",
    "log_data = np.log(np.abs(ts_1d) + 1)\n",
    "# log_data = np.log(ts_1d)  # in a perfect world...\n",
    "x = np.column_stack([np.ones(n_echoes), -1 * echo_times])\n",
    "X = np.repeat(x, n_trs, axis=0)  # T * E\n",
    "\n",
    "# Model fit\n",
    "betas = np.linalg.lstsq(X, log_data, rcond=None)[0]\n",
    "s0 = np.exp(betas[0])\n",
    "r2s = betas[1]\n",
    "t2s = 1. / r2s\n",
    "\n",
    "# Values for plots\n",
    "# Values from log-linear model\n",
    "log_x = np.arange(-1000, 0, .01)\n",
    "log_y = betas[0] + log_x*betas[1]\n",
    "\n",
    "# Values from monoexponential decay model\n",
    "mono_x = np.arange(0, 1000, .01)\n",
    "mono_y = np.exp(-1*betas[1]*mono_x) * s0\n",
    "\n",
    "# Get weights for optimal combination\n",
    "alpha = echo_times * np.exp(-echo_times / t2s)\n",
    "alpha = alpha / np.sum(alpha)  # unnecessary but good for bar plot below\n",
    "\n",
    "# Combine data across echoes\n",
    "oc_manual = np.average(np.vstack(ts), axis=0, weights=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2013360",
   "metadata": {},
   "source": [
    "## Echo-specific timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d69624",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n_echoes, sharex=True, sharey=False, figsize=(14, 6))\n",
    "for i_echo in range(n_echoes):\n",
    "    axes[i_echo].plot(ts[i_echo], color=pal[i_echo])\n",
    "    axes[i_echo].set_ylabel('{0}ms'.format(echo_times[i_echo]), rotation=0, va='center', ha='right', fontsize=14)\n",
    "    axes[i_echo].set_yticks([])\n",
    "    axes[i_echo].set_xticks([])\n",
    "\n",
    "axes[-1].set_xlabel('Time', fontsize=16)\n",
    "axes[-1].set_xlim(0, len(ts[i_echo])-1)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76670f7",
   "metadata": {},
   "source": [
    "## Echo-specific data and echo time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c313a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "values = [i[0] for i in ts]\n",
    "for i_echo in range(n_echoes):\n",
    "    rep_echo_times = np.ones(n_trs) * echo_times[i_echo]\n",
    "    ax.scatter(rep_echo_times, ts[i_echo], alpha=0.05, color=pal[i_echo])\n",
    "\n",
    "ax.set_ylabel('BOLD signal', fontsize=16)\n",
    "ax.set_xlabel('Echo Time (ms)', fontsize=16)\n",
    "ax.set_xticks(echo_times)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_ylim(0, 16000)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b27790",
   "metadata": {},
   "source": [
    "## Adaptive mask\n",
    "Longer echo times are more susceptible to signal dropout, which means that certain brain regions (e.g., orbitofrontal cortex, temporal poles) will only have good signal for some echoes. In order to avoid using bad signal from affected echoes in calculating $T_{2}^*$ and $S_{0}$ for a given voxel, `tedana` generates an adaptive mask, where the value for each voxel is the number of echoes with \"good\" signal. When $T_{2}^*$ and $S_{0}$ are calculated below, each voxel's values are only calculated from the first $n$ echoes, where $n$ is the value for that voxel in the adaptive mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tedana.io import load_data, new_nii_like\n",
    "from tedana.utils import make_adaptive_mask\n",
    "from nilearn.masking import compute_epi_mask\n",
    "\n",
    "mask_img = compute_epi_mask(data_files[0])\n",
    "data, img = load_data(data_files, len(echo_times))\n",
    "mask, adaptive_mask = make_adaptive_mask(data, mask=mask_img, getsum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd3841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_mask_img = new_nii_like(img, adaptive_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "palette = sns.color_palette(\"BuGn_r\", 10)\n",
    "plotting.plot_epi(adaptive_mask_img, vmax=8, alpha=1,\n",
    "                  draw_cross=False, colorbar=True,\n",
    "                  cmap='Blues', black_bg=False,\n",
    "                  annotate=False, bg_img=None, figure=fig, axes=ax)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb50a9b",
   "metadata": {},
   "source": [
    "## Log-linear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbc8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "for i_echo in range(n_echoes):\n",
    "    rep_echo_times = -1 * np.ones(n_trs) * echo_times[i_echo]\n",
    "    log_echo_data = np.log((np.abs(ts[i_echo]) + 1))\n",
    "    ax.scatter(rep_echo_times, log_echo_data, alpha=0.05, color=pal[i_echo])\n",
    "\n",
    "ax.set_ylabel('log(BOLD signal)', fontsize=16)\n",
    "ax.set_xlabel('Negative Echo Time (ms)', fontsize=16)\n",
    "ax.set_xticks(-1 * echo_times)\n",
    "ax.set_xlim(-100, 0)\n",
    "ax.set_ylim(7, 10)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f884c",
   "metadata": {},
   "source": [
    "## Log-linear model\n",
    "Let $S$ be the BOLD signal for a given echo.\n",
    "\n",
    "Let $TE$ be the echo time in milliseconds.\n",
    "\n",
    "$$\\log_{e}(\\left|\\begin{pmatrix}\n",
    "S(TE_{1}) \\\\\n",
    "S(TE_{2}) \\\\\n",
    "\\vdots \\\\\n",
    "S(TE_{n})\\end{pmatrix}\\right|\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1\\end{pmatrix}\n",
    ") = B_{1}\n",
    "\\begin{pmatrix}\n",
    "-TE_{1} \\\\\n",
    "-TE_{2} \\\\\n",
    "\\vdots \\\\\n",
    "-TE_{n}\\end{pmatrix} +\n",
    "\\begin{pmatrix}\n",
    "B_{0} \\\\\n",
    "B_{0} \\\\\n",
    "\\vdots \\\\\n",
    "B_{0}\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef96b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "for i_echo in range(n_echoes):\n",
    "    rep_echo_times = -1 * np.ones(n_trs) * echo_times[i_echo]\n",
    "    log_echo_data = np.log((np.abs(ts[i_echo]) + 1))\n",
    "    ax.scatter(rep_echo_times, log_echo_data, alpha=0.05, color=pal[i_echo])\n",
    "\n",
    "ax.plot(log_x, log_y)\n",
    "\n",
    "ax.set_ylabel('log(BOLD signal)', fontsize=16)\n",
    "ax.set_xlabel('Negative Echo Time (ms)', fontsize=16)\n",
    "ax.set_xticks(-1 * echo_times)\n",
    "ax.set_xlim(-100, 0)\n",
    "ax.set_ylim(7, 10)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "ax.annotate('$B_0$: {0:.02f}\\n$B_1$: {1:.02f}'.format(betas[0], betas[1]),\n",
    "            xy=(-70, 9.5), fontsize=16,\n",
    "            bbox=dict(fc=\"white\", ec=\"black\", lw=1))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9a46a",
   "metadata": {},
   "source": [
    "# Monoexponential decay model\n",
    "Calculation of $S_{0}$ and $T_{2}^{*}$\n",
    "$$S_{0} = e^{B_{0}}$$\n",
    "\n",
    "$$T_{2}^{*} = \\frac{1}{B_{1}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af18beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "for i_echo in range(n_echoes):\n",
    "    rep_echo_times = np.ones(n_trs) * echo_times[i_echo]\n",
    "    ax.scatter(rep_echo_times, ts[i_echo], alpha=0.05, color=pal[i_echo])\n",
    "\n",
    "ax.plot(mono_x, mono_y)\n",
    "\n",
    "ax.set_ylabel('BOLD signal', fontsize=16)\n",
    "ax.set_xlabel('Echo Time (ms)', fontsize=16)\n",
    "ax.set_xticks(echo_times)\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_ylim(0, 16000)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.annotate('$S_0$: {0:.02f}\\n$T_2^*$: {1:.02f}'.format(s0, t2s),\n",
    "            xy=(86.5, 13500), fontsize=16,\n",
    "            bbox=dict(fc=\"white\", ec=\"black\", lw=1))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06b02ba",
   "metadata": {},
   "source": [
    "## T2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbaab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "for i_echo in range(n_echoes):\n",
    "    rep_echo_times = np.ones(n_trs) * echo_times[i_echo]\n",
    "    ax.scatter(rep_echo_times, ts[i_echo], alpha=0.05, color=pal[i_echo])\n",
    "\n",
    "ax.plot(mono_x, mono_y)\n",
    "\n",
    "ax.axvline(t2s, 0, 1, label='$T_2^*$', color='black', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel('BOLD signal', fontsize=16)\n",
    "ax.set_xlabel('Echo Time (ms)', fontsize=16)\n",
    "ax.set_xticks(np.hstack((echo_times, [np.round(t2s, 1)])))\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_ylim(0, 16000)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.xaxis.get_major_ticks()[-1].set_pad(20)\n",
    "\n",
    "legend = ax.legend(frameon=True, fontsize=16)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eceb79",
   "metadata": {},
   "source": [
    "## Optimal combination weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b1e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.barplot(echo_times, alpha, ax=ax, palette=pal)\n",
    "ax.set_ylabel('Weight', fontsize=16)\n",
    "ax.set_xlabel('Echo Time (ms)', fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ca257",
   "metadata": {},
   "source": [
    "## Optimally combined timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc374fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "for i_echo in range(n_echoes):\n",
    "    rep_echo_times = np.ones(n_trs) * echo_times[i_echo]\n",
    "    ax.scatter(rep_echo_times, ts[i_echo], alpha=0.05, color=pal[i_echo])\n",
    "\n",
    "ax.plot(mono_x, mono_y)\n",
    "\n",
    "# Optimal combination\n",
    "rep_t2s = np.ones(n_trs) * t2s\n",
    "ax.scatter(rep_t2s, oc_manual, alpha=1, color='red', label='Optimally\\ncombined\\ndata')\n",
    "\n",
    "ax.axvline(t2s, 0, 20000, label='$T_2^*$', color='black', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel('BOLD signal', fontsize=16)\n",
    "ax.set_xlabel('Echo Time (ms)', fontsize=16)\n",
    "ax.set_xticks(np.hstack((echo_times, [np.round(t2s, 1)])))\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_ylim(0, 16000)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.xaxis.get_major_ticks()[-1].set_pad(20)\n",
    "\n",
    "legend = ax.legend(frameon=True, fontsize=16)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a4470",
   "metadata": {},
   "source": [
    "## Optimally combined timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93246414",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n_echoes+1, sharex=True, sharey=False, figsize=(14, 6))\n",
    "for i_echo in range(n_echoes):\n",
    "    axes[i_echo].plot(ts[i_echo], color=pal[i_echo])\n",
    "    axes[i_echo].set_ylabel('{0}ms'.format(echo_times[i_echo]), rotation=0, va='center', ha='right', fontsize=14)\n",
    "    axes[i_echo].set_yticks([])\n",
    "    axes[i_echo].set_xticks([])\n",
    "\n",
    "axes[-1].plot(oc_manual, color='red')\n",
    "axes[-1].set_ylabel('Optimally\\ncombined\\ndata', rotation=0, va='center', ha='right', fontsize=14)\n",
    "axes[-1].set_xlabel('Time', fontsize=16)\n",
    "axes[-1].set_yticks([])\n",
    "axes[-1].set_xticks([])\n",
    "axes[-1].set_xlim(0, len(ts[i_echo])-1)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aa648b",
   "metadata": {},
   "source": [
    "## Multi-Echo Principal Components Analysis\n",
    "Optimally combined data are decomposed with PCA.\n",
    "The PCA components are selected according to one of multiple possible approaches.\n",
    "Two possible approaches are a decision tree and a threshold using the percentage of variance explained by each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2144bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, sharex=True, figsize=(14, 6))\n",
    "\n",
    "i = 0\n",
    "axes[0].plot(mepca_mmix[:, i])\n",
    "axes[0].set_title('PCA Component {}'.format(i), fontsize=16)\n",
    "\n",
    "i = 3\n",
    "axes[1].plot(mepca_mmix[:, i])\n",
    "axes[1].set_title('PCA Component {}'.format(i), fontsize=16)\n",
    "\n",
    "i = 100\n",
    "axes[2].plot(mepca_mmix[:, i])\n",
    "axes[2].set_title('PCA Component {}'.format(i), fontsize=16)\n",
    "\n",
    "axes[2].set_xlim(0, mepca_mmix.shape[0]-1)\n",
    "axes[2].set_xticks([])\n",
    "axes[2].set_xlabel('Time', fontsize=16)\n",
    "axes[0].tick_params(axis='both', which='major', labelsize=12)\n",
    "axes[1].tick_params(axis='both', which='major', labelsize=12)\n",
    "axes[2].tick_params(axis='both', which='major', labelsize=12)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce7f79",
   "metadata": {},
   "source": [
    "# Data Whitening\n",
    "The selected components from the PCA are recombined to produce a whitened version of the optimally combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca361d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(oc_red[:, voxel_idx], label='Dimensionally reduced timeseries', zorder=1.)\n",
    "ax.plot(oc_z[:, voxel_idx], label='Original timeseries', alpha=0.5, zorder=0., linewidth=3)\n",
    "legend = ax.legend(frameon=True, fontsize=16, loc='upper right', framealpha=1)\n",
    "ax.set_xlim(0, oc_z.shape[0]-1)\n",
    "ax.set_xticks([])\n",
    "ax.set_xlabel('Time', fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7bb03c",
   "metadata": {},
   "source": [
    "## Multi-Echo Independent Components Analysis\n",
    "The whitened optimally combined data are then decomposed with ICA. The number of ICA components is limited to the number of retained components from the PCA, in order to reflect the true dimensionality of the data.\n",
    "ICA produces a mixing matrix (i.e., timeseries for each component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, sharex=True, figsize=(14, 6))\n",
    "i = 0\n",
    "k = df.loc[i, 'kappa']\n",
    "r = df.loc[i, 'rho']\n",
    "c = df.loc[i, 'classification']\n",
    "axes[0].plot(meica_mmix[:, i])\n",
    "axes[0].set_title('ICA Component {0}; $\\\\kappa$ = {1:.02f}; $\\\\rho$ = {2:.02f}; {3}'.format(i, k, r, c), fontsize=16)\n",
    "\n",
    "i = 3\n",
    "k = df.loc[i, 'kappa']\n",
    "r = df.loc[i, 'rho']\n",
    "c = df.loc[i, 'classification']\n",
    "axes[1].plot(meica_mmix[:, i])\n",
    "axes[1].set_title('ICA Component {0}; $\\\\kappa$ = {1:.02f}; $\\\\rho$ = {2:.02f}; {3}'.format(i, k, r, c), fontsize=16)\n",
    "\n",
    "i = 44\n",
    "k = df.loc[i, 'kappa']\n",
    "r = df.loc[i, 'rho']\n",
    "c = df.loc[i, 'classification']\n",
    "axes[2].plot(meica_mmix[:, i])\n",
    "axes[2].set_title('ICA Component {0}; $\\\\kappa$ = {1:.02f}; $\\\\rho$ = {2:.02f}; {3}'.format(i, k, r, c), fontsize=16)\n",
    "\n",
    "axes[0].set_xlim(0, meica_mmix.shape[0]-1)\n",
    "axes[2].set_xticks([])\n",
    "axes[2].set_xlabel('Time', fontsize=16)\n",
    "axes[0].tick_params(axis='both', which='major', labelsize=12)\n",
    "axes[1].tick_params(axis='both', which='major', labelsize=12)\n",
    "axes[2].tick_params(axis='both', which='major', labelsize=12)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c1b7b",
   "metadata": {},
   "source": [
    "# $R_2$ and $S_0$ Model Fit\n",
    "Linear regression is used to fit the component timeseries to each voxel in each echo from the original, echo-specific data. This results in echo- and voxel-specific betas for each of the components. TE-dependence ($R_2$) and TE-independence ($S_0$) models can then be fit to these betas.\n",
    "\n",
    "These models allow calculation of F-statistics for the $R_2$ and $S_0$ models (referred to as $\\kappa$ and $\\rho$, respectively).\n",
    "\n",
    "Note that the values here are for a single voxel (the highest-weighted one for the component), but $\\kappa$ and $\\rho$ are averaged across voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [0, 3, 44]\n",
    "for i, comp in enumerate(components):  # only generate plots for a few components\n",
    "    comp_voxel_idx = np.where(beta_maps[comp, :] == np.max(beta_maps[comp, :]))[0][0]\n",
    "    # Use weight map to average as fitmodels_direct does\n",
    "    comp_weights = meica_betas[comp, :, comp_voxel_idx]\n",
    "    r2_pred_weights = r2_pred_betas[comp, :, comp_voxel_idx]\n",
    "    s0_pred_weights = s0_pred_betas[comp, :, comp_voxel_idx]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 4))\n",
    "    ax.plot(echo_times, comp_weights, c='black', alpha=0.5, linewidth=5, label='Component PEs')\n",
    "    ax.plot(echo_times, r2_pred_weights, c='blue', label='Predicted T2* model values')\n",
    "    ax.plot(echo_times, s0_pred_weights, c='red', label='Predicted S0 model values')\n",
    "    ax.set_xticks(echo_times)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.set_xlabel('Echo Time (ms)', fontsize=16)\n",
    "    temp = np.hstack((comp_weights, s0_pred_weights, r2_pred_weights))\n",
    "    lim = np.mean(temp) * .05\n",
    "    ax.set_ylim(np.floor(np.min(temp)) - lim, np.ceil(np.max(temp)) + lim)\n",
    "    legend = ax.legend(frameon=True, fontsize=14, ncol=3)\n",
    "    ax.set_title('ICA Component {}'.format(comp), fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773fdf95",
   "metadata": {},
   "source": [
    "# ICA Component Selection and Multi-Echo Denoising\n",
    "A decision tree is applied to $\\kappa$, $\\rho$, and other metrics in order to classify ICA components as TE-dependent (BOLD signal), TE-independent (non-BOLD noise), or neither (to be ignored).\n",
    "\n",
    "The ICA components are fitted to the original (not whitened) optimally combined data with linear regression, which is used to weight the components for construction of the denoised data. The residuals from this regression will thus include the variance that was not included in the PCA-whitened optimally combined data.\n",
    "\n",
    "The ME-DN dataset is constructed from the accepted (BOLD) and ignored components, as well as the residual variance not explained by the ICA.\n",
    "The ME-HK dataset is constructed just from the accepted (BOLD) components. This means that ignored components and residual variance not explained by the ICA are not included in the resulting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_data_z = (dn_data - np.mean(dn_data, axis=0)) / np.std(dn_data, axis=0)\n",
    "hk_data_z = (hk_data - np.mean(hk_data, axis=0)) / np.std(hk_data, axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(3, sharex=True, figsize=(14, 6))\n",
    "axes[0].plot(oc_z[:, voxel_idx], label='Optimally combined')\n",
    "axes[0].set_title('Optimally combined', fontsize=16)\n",
    "\n",
    "axes[1].plot(dn_data_z[:, voxel_idx], label='ME-DN')\n",
    "axes[1].set_title('ME-DN', fontsize=16)\n",
    "\n",
    "axes[2].plot(hk_data_z[:, voxel_idx])\n",
    "axes[2].set_title('ME-HK', fontsize=16)\n",
    "legend = ax.legend(frameon=True)\n",
    "axes[0].set_xlim(0, oc_z.shape[0]-1)\n",
    "axes[2].set_xticks([])\n",
    "axes[2].set_xlabel('Time', fontsize=16)\n",
    "axes[0].tick_params(axis='both', which='major', labelsize=12)\n",
    "axes[1].tick_params(axis='both', which='major', labelsize=12)\n",
    "axes[2].tick_params(axis='both', which='major', labelsize=12)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b37c6",
   "metadata": {},
   "source": [
    "# Post-processing to remove spatially diffuse noise\n",
    "Due to the constraints of ICA, MEICA is able to identify and remove spatially localized noise components, but it cannot identify components that are spread out throughout the whole brain.\n",
    "\n",
    "One of several post-processing strategies may be applied to the ME-DN or ME-HK datasets in order to remove spatially diffuse (ostensibly respiration-related) noise. Methods which have been employed in the past include global signal regression (GSR), T1c-GSR, anatomical CompCor, Go Decomposition (GODEC), and robust PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_t1c_data_z = (dn_t1c_data - np.mean(dn_t1c_data, axis=0)) / np.std(dn_t1c_data, axis=0)\n",
    "hk_t1c_data_z = (hk_t1c_data - np.mean(hk_t1c_data, axis=0)) / np.std(hk_t1c_data, axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(2, sharex=True, figsize=(14, 6))\n",
    "axes[0].plot(dn_t1c_data_z[:, voxel_idx], label='ME-DN T1c')\n",
    "axes[0].plot(dn_data_z[:, voxel_idx], label='ME-DN', alpha=0.5, linewidth=3, zorder=0.)\n",
    "axes[0].set_title('ME-DN', fontsize=16)\n",
    "legend = axes[0].legend(frameon=True, loc='upper right')\n",
    "\n",
    "axes[1].plot(hk_t1c_data_z[:, voxel_idx], label='ME-HK T1c')\n",
    "axes[1].plot(hk_data_z[:, voxel_idx], label='ME-HK', alpha=0.5, linewidth=3, zorder=0.)\n",
    "axes[1].set_title('ME-HK', fontsize=16)\n",
    "legend = axes[1].legend(frameon=True)\n",
    "axes[0].set_xlim(0, oc_z.shape[0]-1)\n",
    "axes[1].set_xticks([])\n",
    "axes[1].set_xlabel('Time', fontsize=16)\n",
    "axes[0].tick_params(axis='both', which='major', labelsize=12)\n",
    "axes[1].tick_params(axis='both', which='major', labelsize=12)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "source_map": [
   12,
   16,
   39,
   42,
   126,
   154,
   158,
   170,
   174,
   189,
   194,
   204,
   208,
   217,
   221,
   237,
   267,
   289,
   297,
   317,
   321,
   342,
   346,
   354,
   358,
   383,
   387,
   404,
   411,
   434,
   439,
   449,
   455,
   486,
   495,
   518,
   528,
   551,
   558
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}