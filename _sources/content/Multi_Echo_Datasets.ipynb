{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db970afb",
   "metadata": {},
   "source": [
    "(content:open-datasets)=\n",
    "# Open Multi-Echo Datasets\n",
    "\n",
    "A number of multi-echo datasets have been made public so far.\n",
    "This list is not necessarily up to date, so please check out OpenNeuro to potentially find more.\n",
    "\n",
    "## Multi-echo fMRI replication sample of autobiographical memory, prospection and theory of mind reasoning tasks\n",
    "\n",
    "**Link**: https://openneuro.org/datasets/ds000210\n",
    "\n",
    "**Citation**: {cite:t}`dupre2016multi`\n",
    "\n",
    "**Brief Description**: 31 participants, with one run of resting-state and four runs of cued SGT task.\n",
    "The multi-echo fMRI data have three echoes (13, 27, 43 ms).\n",
    "Additional data include T1-weighted anatomical scans and physiological data (chest belt and PPG)\n",
    "recorded concurrently with the fMRI scans.\n",
    "\n",
    "## Multi-echo Cambridge\n",
    "\n",
    "**Link**: https://openneuro.org/datasets/ds000258\n",
    "\n",
    "**Citation**: {cite:t}`Kundu2013-po`\n",
    "\n",
    "**Brief Description**: 89 participants, with one run of resting-state.\n",
    "The multi-echo fMRI data have four echoes (12, 28, 44, 60 ms).\n",
    "Additional data include T1-weighted anatomical scans.\n",
    "\n",
    "## Multiband multi-echo imaging of simultaneous oxygenation and flow timeseries for resting state connectivity\n",
    "\n",
    "**Link**: https://openneuro.org/datasets/ds000254\n",
    "\n",
    "**Citation**: {cite:t}`cohen2019improving`\n",
    "\n",
    "**Brief Description**: 13 participants, with one run of bilateral finger-tapping task.\n",
    "The multi-echo fMRI data have four echoes (9.1, 25, 39.6, 54.3 ms).\n",
    "Additional data include T1-weighted anatomical scans and pCASL acquired concurrently with the fMRI scans.\n",
    "\n",
    "## Valence processing differs across stimulus modalities\n",
    "\n",
    "**Link**: https://openneuro.org/datasets/ds001491\n",
    "\n",
    "**Citation**: {cite:t}`DALENBERG2018734`\n",
    "\n",
    "**Brief Description**: 20 participants, with one run of image-viewing and four runs of flavor-experiencing task.\n",
    "The multi-echo fMRI data have three echoes (8.02, 22.03, 36.03 ms).\n",
    "Additional data include T1-weighted anatomical scans.\n",
    "\n",
    "## Cambridge Centre for Ageing Neuroscience (Cam-CAN)\n",
    "\n",
    "**Link**: https://camcan-archive.mrc-cbu.cam.ac.uk/dataaccess/\n",
    "\n",
    "**Citation**: {cite:t}`Shafto2014-lg`\n",
    "\n",
    "**Brief Description**: The Cam-CAN MRI dataset is part of the much larger Cam-CAN research project.\n",
    "649 participants completed the MRI scanning protocol with multi-echo fMRI data.\n",
    "There is one run of multi-echo fMRI data, using a movie-watching task.\n",
    "The multi-echo fMRI data have five echoes (9.4, 21.23, 33.06, 44.89, 56.72 ms).\n",
    "Additional data include T1-weighted anatomical scans, T2-weighted anatomical scans,\n",
    "diffusion-weighted scans, magnetization transfer scans, one run of single-echo,\n",
    "resting-state fMRI, and one run of single-echo, sensorimotor task fMRI.\n",
    "\n",
    "```{warning}\n",
    "Accessing this dataset requires a data access application.\n",
    "```\n",
    "\n",
    "## rt-me-fMRI - A task and resting state dataset for real-time, multi-echo fMRI methods development and validation\n",
    "\n",
    "**Link**: https://doi.org/10.34894/R1TNL8\n",
    "\n",
    "**Citation**: {cite:t}`HEUNIS2021118244`\n",
    "\n",
    "**Brief Description**: 28 participants, with one run of resting-state,\n",
    "one run of emotion-processing task, one run of imagined emotion-processing task,\n",
    "one run of finger-tapping task, and one run of imagined finger-tapping task.\n",
    "The multi-echo fMRI data have three echoes (14, 28, 42 ms).\n",
    "Additional data include T1-weighted anatomical scans and physiological data (chest belt and cardiac)\n",
    "acquired concurrently with the fMRI scans.\n",
    "\n",
    "```{warning}\n",
    "Accessing this dataset requires a data access application.\n",
    "```\n",
    "\n",
    "## EuskalIBUR\n",
    "\n",
    "**Link**: https://openneuro.org/datasets/ds003192\n",
    "\n",
    "**Citation**: {cite:t}`MOIA2021117914`\n",
    "\n",
    "**Brief Description**: 7 participants with 10 sessions each, with one run of breath-holding task.\n",
    "The multi-echo fMRI data have five echoes (10.6, 28.69, 46.78, 64.87, 82.96 ms).\n",
    "Additional data include T1-weighted anatomical scans and physiological data (chest belt, cardiac, exhaled CO2, exhaled O2)\n",
    "acquired concurrently with the fMRI scans.\n",
    "\n",
    "## Le Petit Prince\n",
    "\n",
    "**Link**: https://openneuro.org/datasets/ds003643\n",
    "\n",
    "**Citation**: {cite:t}`Li2021.10.02.462875`\n",
    "\n",
    "**Brief Description**: 111 participants, with nine runs of an auditory narrative task.\n",
    "The multi-echo fMRI data have three echoes, with some participants having TEs of 12.8, 27.5, and 43 ms\n",
    "and others having 10, 25, and 38 ms.\n",
    "Additional data include T1-weighted anatomical scans.\n",
    "\n",
    "The auditory narrative task involves listening to Le Petit Prince in nine 10-minute runs, totally around 90 minutes.\n",
    "Given the naturalistic nature of this task, it is a good target for functional alignment analyses.\n",
    "\n",
    "Additionally, the language in which the task was presented differed across participants.\n",
    "51 participants listened to the audiobook in English, 35 listened in Chinese, and 30 listened in French."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "source_map": [
   12
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}